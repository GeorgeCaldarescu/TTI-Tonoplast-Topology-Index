{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessaries libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## Function that import the csv from the current directory files as dataframes and store them as a DataFrame in a dictionary ######################## \n",
    "\n",
    "def import_csv_to_dataframes(directory):\n",
    "    \"\"\" \n",
    "    Reads all CSV files from the current directory and store them as pandas DataFrames in a dictionary called dataframes.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to the directory containing CSV files.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are filename (without extension) and values are pandas DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    files = os.listdir(directory)  # List all the files in the directory\n",
    "    \n",
    "    csv_files = [file for file in files if file.endswith('.csv')]  # Filter out CSV files\n",
    "    \n",
    "    dataframes = {} # Create empty dictionary to store the DataFrames\n",
    "    \n",
    "    for csv_files in csv_files:\n",
    "        df_name = os.path.splitext(csv_files)[0] # Extract filename without extansion\n",
    "        dataframe = pd.read_csv(os.path.join(directory, csv_files)) #Read CSV files\n",
    "        dataframes[df_name] = dataframe # Store Dataframes in dictionary\n",
    "    return dataframes\n",
    "           \n",
    "\n",
    "######################## Function to detect and plot peaks from the DataFrames containing _spectra_ ########################\n",
    "def plot_profile(dataframes, background=0, med_ratio=1.4, min_ratio=1.4, distance=0.1, prominence=0.1, sigma=0.01):\n",
    "    \"\"\"\n",
    "    Identifies peaks in spectral data and visualise the detected peaks.\n",
    "    \n",
    "    Args:\n",
    "        dataframes (dict): dictionary of DataFrames containing spectral data.\n",
    "        background (flaot): minimum background threshold for peak detection.\n",
    "        med_ratio (float): median-based filtering ratio.\n",
    "        min_ratio (float): minimum peak value filtering ratio.\n",
    "        distance (float): minimum distance between peaks.\n",
    "        prominence (float): minimum prominence of peaks.\n",
    "        sigma (float): smoothing factor for Gaussian filter\n",
    "    \n",
    "    Returns:\n",
    "        peaks_dict: dictionary containing peak information (indices, distance, and values) for each DataFrame.\n",
    "    \"\"\"\n",
    "    peaks_dict = {}\n",
    "    \n",
    "    for key, df in dataframes.items():\n",
    "        if 'spectra' in key:\n",
    "            # Extract distance and intensity values\n",
    "            distance_list = df['Distance'].tolist() \n",
    "            value = df['Value'].tolist()\n",
    "\n",
    "            # Use the median intensity to define a threshold for valid peaks\n",
    "            median_value = np.median(value)\n",
    "            threshold = max(background, med_ratio * median_value)\n",
    "            \n",
    "            # Apply Gaussian smoothing to reduce noise and make peak detection more robust\n",
    "            smoothed_value = gaussian_filter1d(value, sigma=sigma)\n",
    "            \n",
    "            # Identify peaks using scipy's find_peak function\n",
    "            peaks, properties = find_peaks(smoothed_value, height=threshold, distance=distance, prominence=prominence)\n",
    "\n",
    "            # Further filter peaks to exclude weak signal\n",
    "            filtered_indices = [i for i in peaks if value[i] > med_ratio * median_value]\n",
    "            filtered_indices = [i for i in filtered_indices if value[i] > min_ratio * min(value)]\n",
    "            filtered_indices = [i for i in filtered_indices if value[i] > background]\n",
    "\n",
    "            # Store peak indices, distance, and values in a dictionary\n",
    "            peaks_dict[key] = {\n",
    "                'peaks_indices': filtered_indices,\n",
    "                'peaks_distances': np.array(distance_list)[filtered_indices].tolist(),\n",
    "                'peak_value': np.array(value)[filtered_indices].tolist()\n",
    "            }\n",
    "            \n",
    "            # Plot original and smothed signal with detected peak\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(distance_list, value, label='Original Value', alpha=0.5)  \n",
    "            plt.plot(distance_list, smoothed_value, label='Smoothed Value', linewidth=2)  # Smoothed signal\n",
    "\n",
    "            \n",
    "            # Scatter plot of the peaks with red crosses\n",
    "            plt.scatter(np.array(distance_list)[filtered_indices], np.array(value)[filtered_indices], color='red', marker='x', label='Peaks')\n",
    "            plt.title(f\"{key} - Min: {min(value):.3f}, Median: {median_value:.3f}, Max: {max(value):.3f}, Peaks: {len(filtered_indices)}\")\n",
    "            plt.xlabel('Distance')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "    return peaks_dict\n",
    "\n",
    "\n",
    "######################## Function to generate the final output DataFrame from extracted peak information ########################\n",
    "def populate_df_from_dict(final_df, dataframes, peaks_dict):\n",
    "    \"\"\" \n",
    "    Populate the final output table \n",
    "    \n",
    "    Args:\n",
    "        final_df (pd.Dataframe): existing DataFrame created in the beginning to apend results to.\n",
    "        dataframes (dict): dictionary of the imported csv files.\n",
    "        peaks_dict (dict): dictionary containing peak detection results\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated final_df with the information regarding the cells\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for key in dataframes:\n",
    "        if 'spectra' in key:\n",
    "            parts = key.split('_')\n",
    "            if len(parts) >= 4:\n",
    "                genotype, spectra, sample, cell = parts[0], parts[1], parts[2], parts[3]\n",
    "                \n",
    "                # Look in the peak_dict for the correct key so I can add the peak count in the correct row\n",
    "                peak_count = len(peaks_dict[key]['peaks_indices']) if key in peaks_dict else None\n",
    "                \n",
    "                # Calculate the Transect length as the last Distance value\n",
    "                transect = None  \n",
    "                if not dataframes[key].empty and 'Distance' in dataframes[key].columns:\n",
    "                    transect = dataframes[key]['Distance'].iloc[-1]  # Take last value instead of max()\n",
    "                    \n",
    "                # Add the cell length value\n",
    "                length_key = f\"{genotype}_length_{sample}\"\n",
    "                length = None  \n",
    "                if length_key in dataframes and not dataframes[length_key].empty:\n",
    "                    cell_index = int(cell) - 1  # Convert cell to int\n",
    "                    if cell_index < len(dataframes[length_key]):\n",
    "                        length = dataframes[length_key].iloc[cell_index]['Length']\n",
    "                    \n",
    "                # Populate the empty df with new rows\n",
    "                new_row = {\n",
    "                    'Sample': genotype,\n",
    "                    'Image': sample,\n",
    "                    'Cell': cell,\n",
    "                    'Length': length,\n",
    "                    'TTI': peak_count / transect if transect else None,  # Avoid division by None\n",
    "                    'Transect Length': transect,\n",
    "                    'Peaks Count': peak_count\n",
    "                }\n",
    "                rows.append(new_row)\n",
    "    \n",
    "    new_df = pd.DataFrame(rows, columns=final_df.columns)\n",
    "    result_df = pd.concat([final_df, new_df], ignore_index=True)\n",
    "    \n",
    "    # Sort the columns in order to have them from A to Z\n",
    "    result_df = result_df.sort_values(by=['Sample', 'Image', 'Cell'], ascending=[True, True, True]).reset_index(drop=True)\n",
    "    \n",
    "    # Transform the peak columns to numeric\n",
    "    result_df['Peaks Count'] = pd.to_numeric(result_df['Peaks Count'], errors='coerce')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the empty dataframe that will be used to store the final data\n",
    "columns = ['Sample', 'Image', 'Cell', 'Length', 'TTI', 'Transect Length', 'Peaks Count']\n",
    "\n",
    "final_df = pd.DataFrame(columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing files\n",
    "dataframes = import_csv_to_dataframes('.')\n",
    "# List the keys of the dictionary\n",
    "dataframe_keys = list(dataframes.keys())\n",
    "print(dataframe_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for plot_profile function\n",
    "background = 0\n",
    "med_ratio = 0.9\n",
    "min_ratio = 1.4\n",
    "distance = 1\n",
    "prominence = 0.1 # lower if valid peaks are missing\n",
    "sigma = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the peaks\n",
    "peaks_dict = plot_profile(dataframes, background=background, med_ratio=med_ratio, min_ratio=min_ratio, distance=distance, prominence=prominence, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final Dataframe\n",
    "\n",
    "final_df = populate_df_from_dict(final_df, dataframes, peaks_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data are randomised or not\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(final_df['Length'], final_df['Transect Length'], c = 'blue')\n",
    "\n",
    "# Add labels and titles\n",
    "plt.xlabel('Cell length')\n",
    "plt.ylabel('Transect length')\n",
    "plt.title('Scatter plot of Transect Lenght vs Cell Length')\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the output as excel\n",
    "final_df.to_excel(\"output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
